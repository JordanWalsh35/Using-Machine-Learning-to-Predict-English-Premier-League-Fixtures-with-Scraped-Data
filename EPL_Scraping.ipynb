{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from lxml import html\n",
    "import time\n",
    "import difflib\n",
    "\n",
    "# Turn off SSL Warning\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "# Define URL & Header\n",
    "curr_season = '2022-2023'\n",
    "transfer_season = curr_season[0:4] \n",
    "url = 'https://fbref.com/en/comps/9/{0}/{1}-Premier-League-Stats'.format(curr_season, curr_season)\n",
    "header_ = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\", \"Accept-Language\": \"en-US,en;q=0.9\"}\n",
    "\n",
    "# Make Request to URL\n",
    "req = requests.get(url, headers = header_, verify = False)\n",
    "\n",
    "# Parse HTML With BeautifulSoup\n",
    "dat = BeautifulSoup(req.text)\n",
    "league_table = dat.select('table.stats_table')[0]\n",
    "\n",
    "# Find Links in HTML Text\n",
    "team_links = league_table.find_all('a')\n",
    "team_links = [link.get('href') for link in team_links]\n",
    "team_links = [link for link in team_links if '/squads/' in link]\n",
    "\n",
    "# Complete Each Link By Adding Prefix\n",
    "pre = 'https://fbref.com'\n",
    "team_links = [pre + link for link in team_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Data from Transfermarkt.co.uk\n",
    "urlT = 'https://www.transfermarkt.co.uk/premier-league/transfers/wettbewerb/GB1/plus/?saison_id={}&s_w=&leihe=0&intern=0&intern=1'.format(transfer_season)\n",
    "trans_req = requests.get(urlT, headers = header_, verify = False)\n",
    "\n",
    "# Parse HTML & Find Span Classes Containing Expenditures\n",
    "transers = BeautifulSoup(trans_req.text)\n",
    "transfer_data = transers.find_all(\"span\", {\"class\":\"transfer-einnahmen-ausgaben redtext\"})\n",
    "title_tags = transers.find_all(\"h2\")[0:20]\n",
    "tnames = [str(tag).split('\"')[-2] for tag in title_tags]\n",
    "expenditures = []\n",
    "\n",
    "# Clean-Up Expenditure Data\n",
    "for x in transfer_data:\n",
    "    expenditures.append(x.text[17:22])\n",
    "for i in range(0, 20):\n",
    "    if expenditures[i] == '\\t\\t\\t\\t\\t':\n",
    "        expenditures[i] = 0\n",
    "    if 'm' in str(expenditures[i]):\n",
    "        expenditures[i] = expenditures[i].replace(\"m\", \"\")\n",
    "    if 'Th' in str(expenditures[i]):\n",
    "        expenditures[i] = expenditures[i].replace(\"Th\", \"\")\n",
    "        expenditures[i] = float(expenditures[i])/1000\n",
    "    expenditures[i] = float(expenditures[i])\n",
    "\n",
    "# Create Dataframe for Each Team's Transfer Expenses\n",
    "team_transfers = pd.DataFrame(data = tnames, index = range(0,20), columns = [\"Team\"])\n",
    "team_transfers[\"Expenditures\"] = expenditures\n",
    "\n",
    "# Sort & Match Team Names\n",
    "tn = []\n",
    "names = [u.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") for u in team_links]\n",
    "\n",
    "for i in range(0, 20):\n",
    "    check_word = team_transfers[\"Team\"][i]\n",
    "    n = 1\n",
    "    cutoff = 0.8\n",
    "    close_match = difflib.get_close_matches(check_word, names, n, cutoff)\n",
    "    team_transfers[\"Team\"].iat[i] = close_match[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = []\n",
    "\n",
    "# Loop Through Each Team URL\n",
    "for url in team_links:\n",
    "    team_req = requests.get(url, headers = header_, verify = False)\n",
    "    team_name = url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    for i in range(0, len(team_transfers)):\n",
    "        if team_name == team_transfers[\"Team\"][i]:\n",
    "            team_exp = team_transfers[\"Expenditures\"][i]\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Use Pandas to Read the Scores & Fixtures Table for Each Team\n",
    "    try:\n",
    "        scores_fixtures = pd.read_html(team_req.text, match = \"Scores & Fixtures\")[0]\n",
    "    except:\n",
    "        scores_fixtures = pd.read_html(team_req.text, match = \"Scores & Fixtures \")[0]\n",
    "    scores_fixtures = scores_fixtures[scores_fixtures[\"Comp\"] == \"Premier League\"]\n",
    "    scores_fixtures.insert(loc = 9, column = \"Team\", value = team_name)\n",
    "    scores_fixtures.insert(loc = 19, column = \"Transfers - Home\", value = team_exp)\n",
    "    scores_fixtures = scores_fixtures.drop(columns = [\"Comp\", \"xG\", \"xGA\", \"Attendance\", \"Captain\", \"Referee\", \"Match Report\", \"Notes\"])\n",
    "\n",
    "    # Parse HTML With BeautifulSoup & Find Links\n",
    "    data = BeautifulSoup(team_req.text)\n",
    "    squad_links = data.find_all('a')\n",
    "    squad_links = [link.get('href') for link in squad_links]\n",
    "    squad_links = [link for link in squad_links if link and 'matchlogs/all_comps/' in link]\n",
    "    squad_links = list(dict.fromkeys(squad_links))\n",
    "    squad_links = squad_links[1:]\n",
    "\n",
    "    # Loop Through Extra Tables\n",
    "    stats_list = []\n",
    "    for link in squad_links:\n",
    "        table_page = requests.get(pre + link, headers = header_, verify = False)\n",
    "        table = pd.read_html(table_page.text)[0]\n",
    "        table.columns = table.columns.droplevel()\n",
    "        table = table[table[\"Comp\"] == \"Premier League\"]\n",
    "        if \"/shooting/\" not in link:\n",
    "            table = table.drop(columns = \"Date\")\n",
    "        if \"/shooting/\" in link:\n",
    "            table = table.rename(columns={'Sh':'Shots', 'FK':'FK Shots'})\n",
    "        if \"/passing_types/\" in link:\n",
    "            table = table.rename(columns={'Live':'Live Passes', 'Cmp':'Passes Completed', 'Press':'Passes Under Pressure'})\n",
    "        stats_list.append(table)\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Combine Tables for Current Team\n",
    "    combined = pd.concat(stats_list, axis = 1)\n",
    "    combined = combined[[\"Date\",\"Shots\", \"SoT%\", \"G/SoT\", \"Dist\", \"FK Shots\", \"PK\", \"SoTA\", \"Save%\", \"CS\", \"PKA\", \"#OPA\", \"Live Passes\", \"Passes Completed\", \"Passes Under Pressure\", \n",
    "                        \"Ground\", \"High\", \"SCA\", \"Press\", \"Succ%\", \"CrdR\", \"CrdY\", \"Fls\", \"OG\", \"Won%\"]]\n",
    "    team_stats = scores_fixtures.merge(combined, on = \"Date\")\n",
    "    all_stats.append(team_stats)\n",
    "\n",
    "# Create Dataframe for All Teams and Save to CSV\n",
    "df = pd.concat(all_stats)\n",
    "df = df.rename(columns = {'Poss':'Possession', 'SoTA':'SoT Against', 'Dist':'Avg. Distance of Shots', 'CS':'Clean Sheet', 'PKA':'PK Allowed', '#OPA':'# Defensive Actions OPA', 'Ground':'Ground Passes',\n",
    "                        'High':'High Passes', 'SCA':'Shot-Creating Actions', 'Press':'Presses (Applied)','Succ%':'Successful Dribbles', 'CrdR':'Red Cards', 'CrdY':'Yellow Cards',\n",
    "                        'Fls':'Fouls', 'OG':'Own Goals', 'Won%':'Aerial Battles Won (%)'})\n",
    "df.to_csv(\"{}.csv\".format(curr_season))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52192ef22709f95f89610121f4a1d8588d161d31b8ddbff6c146c7538d5f4a3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
